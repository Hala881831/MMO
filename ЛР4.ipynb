{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Лабораторная работа 4**"
      ],
      "metadata": {
        "id": "kuQVRd_GvZUA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Хаммуд Хала ИУ5И-21М**"
      ],
      "metadata": {
        "id": "P0YZI1SNvpSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Реализация алгоритма Policy Iteration**"
      ],
      "metadata": {
        "id": "qT00E1McBjA8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание:** На основе рассмотренного на лекции примера реализуйте алгоритм Policy Iteration для любой среды обучения с подкреплением (кроме рассмотренной на лекции среды Toy Text / Frozen Lake) из библиотеки Gym (или аналогичной библиотеки)."
      ],
      "metadata": {
        "id": "bxgPX6oCBlU0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Описание задания**"
      ],
      "metadata": {
        "id": "JkN70zWGFecf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В рамках лабораторной работы №4 по теме «Обучение на основе алгоритма Policy Iteration» необходимо реализовать базовый метод обучения с подкреплением — итерацию стратегии (policy iteration) — в символьной среде. В качестве среды используется Taxi-v3 из библиотеки Gymnasium, которая имитирует дискретную модель управления действиями агента в ограниченном пространстве.\n",
        "\n",
        "Особенностью данной реализации является интерпретация среды как условной модели поведения человека с разным уровнем усталости. Агент, построенный в ходе работы, получает и обновляет вероятностную стратегию выбора действий в зависимости от состояния, имитируя когнитивную активность утомлённого или внимательного пользователя.\n",
        "\n",
        "Цель задания —:\n",
        "\n",
        "реализовать алгоритм итерации стратегии (policy evaluation и policy improvement);\n",
        "\n",
        "обучить агента на основе среды Taxi-v3;\n",
        "\n",
        "интерпретировать полученную стратегию как поведение человека с учётом уровня усталости;\n",
        "\n",
        "визуализировать или вывести стратегию агента после обучения;\n",
        "\n",
        "при возможности — продемонстрировать поведение обученного агента.\n",
        "\n"
      ],
      "metadata": {
        "id": "6c_R9_yAFh9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-RReIl3CUje",
        "outputId": "df0f760e-b6bc-4c62-bbbf-6687e3802990"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "4yng_LxICZcj"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FatiguePolicyAgent:\n",
        "    '''\n",
        "    Агент, имитирующий принятие решений человеком с разным уровнем усталости.\n",
        "    '''\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        self.observation_dim = env.observation_space.n\n",
        "        self.actions_variants = np.array(range(env.action_space.n))  # возможные действия\n",
        "\n",
        "        # Начальная стратегия — случайное поведение (можно трактовать как уставшее состояние)\n",
        "        self.policy_probs = np.full((self.observation_dim, len(self.actions_variants)), 1.0 / len(self.actions_variants))\n",
        "\n",
        "        # Начальные значения функции состояния\n",
        "        self.state_values = np.zeros(shape=(self.observation_dim))\n",
        "\n",
        "        # Параметры обучения\n",
        "        self.max_iterations = 1000\n",
        "        self.theta = 1e-6\n",
        "        self.gamma = 0.99\n",
        "\n",
        "    def policy_evaluation(self):\n",
        "        '''\n",
        "        Этап оценивания стратегии — насколько \"эффективен\" уставший или внимательный агент\n",
        "        '''\n",
        "        for iteration in range(self.max_iterations):\n",
        "            new_state_values = np.zeros_like(self.state_values)\n",
        "            for state in range(self.observation_dim):\n",
        "                action_probs = self.policy_probs[state]\n",
        "                state_value = 0\n",
        "                for action, action_prob in enumerate(action_probs):\n",
        "                    for prob, next_state, reward, done in self.env.P[state][action]:\n",
        "                        state_value += action_prob * prob * (reward + self.gamma * self.state_values[next_state])\n",
        "                new_state_values[state] = state_value\n",
        "            # Проверка сходимости\n",
        "            if np.max(np.abs(new_state_values - self.state_values)) < self.theta:\n",
        "                break\n",
        "            self.state_values = new_state_values\n",
        "        return self.state_values\n",
        "\n",
        "    def policy_improvement(self):\n",
        "        '''\n",
        "        Этап улучшения стратегии — переход от \"уставшего\" к \"внимательному\" поведению\n",
        "        '''\n",
        "        new_policy = np.zeros_like(self.policy_probs)\n",
        "        for state in range(self.observation_dim):\n",
        "            q_values = np.zeros(len(self.actions_variants))\n",
        "            for action in self.actions_variants:\n",
        "                for prob, next_state, reward, done in self.env.P[state][action]:\n",
        "                    q_values[action] += prob * (reward + self.gamma * self.state_values[next_state])\n",
        "            # Выбираем лучшее действие\n",
        "            best_actions = np.where(q_values == np.max(q_values))[0]\n",
        "            new_policy[state][best_actions] = 1.0 / len(best_actions)\n",
        "        return new_policy\n",
        "\n",
        "    def policy_iteration(self, iterations=20):\n",
        "        '''\n",
        "        Основной цикл — повторяем оценку и улучшение, пока не достигнем \"устойчивого поведения\"\n",
        "        '''\n",
        "        for i in range(iterations):\n",
        "            self.policy_evaluation()\n",
        "            self.policy_probs = self.policy_improvement()\n",
        "        print(f'Policy Iteration завершён после {iterations} шагов.')\n",
        "        from pprint import pprint\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iwLs1vDsCaxK"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_agent(agent):\n",
        "    import time\n",
        "    env2 = gym.make('Taxi-v3', render_mode='human')\n",
        "    state, _ = env2.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        p = agent.policy_probs[state]\n",
        "        action = np.random.choice(len(agent.actions_variants), p=p)\n",
        "        next_state, reward, terminated, truncated, _ = env2.step(action)\n",
        "        total_reward += reward\n",
        "        env2.render()\n",
        "        time.sleep(0.5)\n",
        "        state = next_state\n",
        "        done = terminated or truncated\n",
        "    print(f\"\\nОбщий полученный reward: {total_reward}\")\n",
        "    env2.close()\n"
      ],
      "metadata": {
        "id": "mSr61heQC1r_"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "agent = FatiguePolicyAgent(env)\n",
        "agent.policy_iteration(20)\n",
        "pprint(agent.policy_probs)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQfA5B_iDSUJ",
        "outputId": "36695ff5-7b6c-40fe-e73c-fcd478247e5b"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Iteration завершён после 20 шагов.\n",
            "array([[0. , 0. , 0. , 0. , 1. , 0. ],\n",
            "       [0. , 0. , 0. , 0. , 1. , 0. ],\n",
            "       [0. , 0. , 0. , 0. , 1. , 0. ],\n",
            "       ...,\n",
            "       [0. , 1. , 0. , 0. , 0. , 0. ],\n",
            "       [0. , 0.5, 0. , 0.5, 0. , 0. ],\n",
            "       [0. , 0. , 0. , 1. , 0. , 0. ]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pygame"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZZLoB8zE90K",
        "outputId": "8bb11e96-ac51-455d-84ab-a5677aaccf8b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def play_agent(agent):\n",
        "    '''\n",
        "    Проигрывание поведения агента в консоли (без графики)\n",
        "    '''\n",
        "    env2 = gym.make('Taxi-v3')\n",
        "    state, _ = env2.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    while not done:\n",
        "        p = agent.policy_probs[state]\n",
        "        action = np.random.choice(len(agent.actions_variants), p=p)\n",
        "        next_state, reward, terminated, truncated, _ = env2.step(action)\n",
        "        total_reward += reward\n",
        "        print(f\"Шаг {steps}: состояние={state}, действие={action}, награда={reward}\")\n",
        "        state = next_state\n",
        "        steps += 1\n",
        "        done = terminated or truncated\n",
        "    print(f\"\\nОбщая награда: {total_reward}, всего шагов: {steps}\")\n",
        "    env2.close()\n"
      ],
      "metadata": {
        "id": "TGsBRHbbFLQG"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Выводы**"
      ],
      "metadata": {
        "id": "Z_9pe-BtFxXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В ходе выполнения лабораторной работы был реализован алгоритм итерации стратегии (Policy Iteration) для дискретной среды Taxi-v3 из библиотеки Gymnasium. Алгоритм состоял из двух ключевых этапов — оценки текущей стратегии (policy evaluation) и её улучшения (policy improvement). Для хранения вероятностной стратегии использовалась матрица вероятностей действий в каждом состоянии среды.\n",
        "\n",
        "Особенностью данной реализации является её интерпретация в рамках магистерского проекта, посвящённого определению уровня усталости человека с использованием мобильных вычислительных устройств. Среда Taxi-v3 была условно рассмотрена как модель когнитивного поведения, где каждое состояние и действие агента может быть связано с проявлениями утомления или внимательности. Вероятностная стратегия, формируемая агентом в процессе итераций, может быть интерпретирована как динамика принятия решений пользователем с различной степенью усталости.\n",
        "\n",
        "В результате эксперимента агент обучился принимать более рациональные действия, минимизируя количество шагов до достижения цели. Это подтверждается улучшением функции ценности состояний и сходимостью алгоритма после заданного числа итераций.\n",
        "\n",
        "Таким образом, полученные результаты демонстрируют потенциальную применимость базовых алгоритмов обучения с подкреплением для задач моделирования и анализа поведенческих стратегий человека в условиях когнитивной нагрузки и усталости.\n",
        "\n"
      ],
      "metadata": {
        "id": "P8_EnqCmFyrQ"
      }
    }
  ]
}